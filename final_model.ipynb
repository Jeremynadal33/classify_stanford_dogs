{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "final_model.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "progressive-crawford"
      },
      "source": [
        "# This notebook presents a baseline cnn model trained with and without data augmentation to see the differences\n",
        "\n",
        "\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Jeremynadal33/classify_stanford_dogs/blob/master/final_model.ipynb)\n",
        "\n",
        "The aim of this notebook is : \n",
        "* Present best model \n",
        "* Tune its hyper-para\n",
        "* Save the model \n",
        "* Write a function that predicts the breed of a dog given its photo \n",
        "\n",
        "\n",
        "The hyper parameter search is done using Skopt\n"
      ],
      "id": "progressive-crawford"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "consecutive-basis",
        "outputId": "e4ad6aa2-989f-4e6b-eb78-c80d0dc0cef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img, load_img\n",
        "from tensorflow.keras import backend as K\n",
        "import os \n",
        "import shutil\n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib import image\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(tf.__version__)"
      ],
      "id": "consecutive-basis",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz-fxwsV6HRO",
        "outputId": "50b05b80-985f-424e-ac38-10c31b0ebd3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "  print('Running on CoLab')\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive',force_remount=True)\n",
        "  !ls /content/gdrive/My\\ Drive/Formation-OC/P5-Stackoverflow\n",
        "  root_dir = '/content/gdrive/My Drive/Formation-OC/P6-Images/'\n",
        "  input_dir = root_dir + 'inputs/'\n",
        "  png_dir = root_dir + 'pngs/'\n",
        "\n",
        "  baseline_species = os.listdir(input_dir+'baseline_inputs/train')\n",
        "\n",
        "  model_dir = root_dir +'models/'\n",
        "\n",
        "  #my script\n",
        "  !ls gdrive/MyDrive/Formation-OC/P6-Images/\n",
        "else:\n",
        "  print('Not running on CoLab')\n",
        "  #my script\n",
        "  root_dir = '/Users/jeremynadal/Documents/Formation OC IML/P6/'\n",
        "  input_dir = root_dir + 'inputs/'\n",
        "  png_dir = root_dir + 'pngs/'\n",
        "  model_dir = root_dir +'models/'"
      ],
      "id": "Jz-fxwsV6HRO",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on CoLab\n",
            "Mounted at /content/gdrive\n",
            "function.py  inputs  P5_presentation.pptx  pngs\n",
            "inputs\tmodels\tP6-presentation.pptx  pngs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9Bo9yp_6MlN"
      },
      "source": [
        "np.random.seed(42) # To ensure re-usability\n",
        "baseline_dir = input_dir+'baseline_inputs/'"
      ],
      "id": "j9Bo9yp_6MlN",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaQHaqJZC-aw"
      },
      "source": [
        "## Scikit optimize is not natively installed :"
      ],
      "id": "qaQHaqJZC-aw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1s14viUDBuK",
        "outputId": "ddb6f73d-5f7b-4ff7-cc33-d6a2e77f9e0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install h5py scikit-optimize\n"
      ],
      "id": "S1s14viUDBuK",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Collecting scikit-optimize\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8b/03/be33e89f55866065a02e515c5b319304a801a9f1027a9b311a9b1d1f8dc7/scikit_optimize-0.8.1-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize) (1.0.0)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-20.4.0 scikit-optimize-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4p8HTwUDLS6",
        "outputId": "c0d349a2-64c4-491a-ab20-3f6e6c7bde24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pip install git+git://github.com/Hvass-Labs/scikit-optimize.git@dd7433da068b5a2509ef4ea4e5195458393e6555"
      ],
      "id": "U4p8HTwUDLS6",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/Hvass-Labs/scikit-optimize.git@dd7433da068b5a2509ef4ea4e5195458393e6555\n",
            "  Cloning git://github.com/Hvass-Labs/scikit-optimize.git (to revision dd7433da068b5a2509ef4ea4e5195458393e6555) to /tmp/pip-req-build-wjk3attq\n",
            "  Running command git clone -q git://github.com/Hvass-Labs/scikit-optimize.git /tmp/pip-req-build-wjk3attq\n",
            "  Running command git checkout -q dd7433da068b5a2509ef4ea4e5195458393e6555\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.5) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.5) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.5) (0.22.2.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from scikit-optimize==0.5) (3.2.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scikit-optimize==0.5) (1.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-optimize==0.5) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-optimize==0.5) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-optimize==0.5) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->scikit-optimize==0.5) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->scikit-optimize==0.5) (1.15.0)\n",
            "Building wheels for collected packages: scikit-optimize\n",
            "  Building wheel for scikit-optimize (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-optimize: filename=scikit_optimize-0.5-py2.py3-none-any.whl size=77763 sha256=ea5f3441056b1bf6efde6efdc35590295c04f51e691b6f6bf20515a910e00a3f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jpp6rnxu/wheels/e2/8b/7e/2956a87f1fb737a8c3f2ed18788b5e0ecfe4718141988c838f\n",
            "Successfully built scikit-optimize\n",
            "Installing collected packages: scikit-optimize\n",
            "  Found existing installation: scikit-optimize 0.8.1\n",
            "    Uninstalling scikit-optimize-0.8.1:\n",
            "      Successfully uninstalled scikit-optimize-0.8.1\n",
            "Successfully installed scikit-optimize-0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSrVaQqBClMP",
        "outputId": "3cd3d5ba-213c-41d7-8a96-85092770eb60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Imports for skopt\n",
        "import skopt\n",
        "from skopt import gp_minimize, forest_minimize\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "from skopt.plots import plot_convergence\n",
        "from skopt.plots import plot_objective, plot_evaluations\n",
        "from skopt.plots import plot_objective\n",
        "from skopt.utils import use_named_args"
      ],
      "id": "KSrVaQqBClMP",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.metrics.scorer module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6doMm2s6l4y"
      },
      "source": [
        "# Prepare everything for the training\n",
        "## Build a function to create a model"
      ],
      "id": "P6doMm2s6l4y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixfE0G-16VCW"
      },
      "source": [
        "def create_cnn(input_shape, output_length,\n",
        "               nb_cnn=3, nb_filters = 64, activation_cnn = 'relu', \n",
        "               model_transfert = None, fine_tune = False, \n",
        "               nb_FC_layer = 3, nb_FC_neurons = 512, reducing = False, activation_FC = 'relu',\n",
        "               dropout = False,\n",
        "               name = 'my_cnn_model'\n",
        "               ):\n",
        "  '''Create a CNN based model is model_transfert is None. Else, the model_transfert is used for feature extraction. \n",
        "  If reducing is not False, nb_FC_neurons must be multiple of 2**nb_FC_layer '''\n",
        "\n",
        "  assert input_shape[-1] == 3, 'For the moment only models with rgb input is dealt'\n",
        "  #for shape in input_shape[:-1] : assert shape % 2**nb_cnn ==  0 , 'Each dimension of input must be a multiple of 2**nb_cnn'\n",
        "  if reducing : assert nb_FC_neurons % 2**nb_FC_layer == 0 , 'If reducing, nb_FC_neurons must be multiple of 2**nb_FC_layer '\n",
        "\n",
        "  model = tf.keras.models.Sequential(name=name)\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=input_shape, name = 'Input_layer'))\n",
        "  model.add( tf.keras.layers.experimental.preprocessing.Rescaling(1./255,name='Rescaling_layer') ),\n",
        "\n",
        "  if model_transfert == None: \n",
        "    for cnn in range(nb_cnn):\n",
        "      model.add(tf.keras.layers.Conv2D( filters = nb_filters, kernel_size = (3,3), padding='same', activation = activation_cnn, name ='Conv2D_'+str(cnn+1) ))\n",
        "      model.add(tf.keras.layers.MaxPooling2D( pool_size=(2, 2), name ='MaxPool_'+str(cnn+1)))\n",
        "  else : \n",
        "    if not fine_tune :\n",
        "      model_transfert.trainable = False\n",
        "    model.add(model_transfert)\n",
        "  \n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "  if reducing : \n",
        "    for FC in range(nb_FC_layer):\n",
        "      model.add(tf.keras.layers.Dense(nb_FC_neurons/2**FC, activation= activation_FC, name='FC_layer_'+str(FC+1)))\n",
        "      if dropout : \n",
        "        model.add(tf.keras.layers.Droupout(dropout, name = 'Dropout_'+str(FC+1)))\n",
        "  else:\n",
        "    for FC in range(nb_FC_layer):\n",
        "      model.add(tf.keras.layers.Dense(nb_FC_neurons, activation= activation_FC, name='FC_layer_'+str(FC+1)))\n",
        "      if dropout :\n",
        "        if dropout != 0.0:  \n",
        "          model.add(tf.keras.layers.Droupout(dropout, name = 'Dropout_'+str(FC+1)))\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(output_length, activation = 'softmax',name='Output_layer'))\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "def get_callbacks(monitor='val_loss',save_name=None,patience=8):\n",
        "    '''Returns the wanted callbacks to save models and avoid overfitting.\n",
        "    monitor (str, optional): the monitor to check for the early stopping. Default is 'val_loss'\n",
        "    save_name (str, optional): if not None, uses modelcheckpoint and saves checkpoints at the save_name. Default is None.\n",
        "    patience (int, optional): number of epoch to wait for improvment of monitor. Default is 8.'''\n",
        "    if save_name :\n",
        "        return [tf.keras.callbacks.ModelCheckpoint(filepath=save_name,\n",
        "                                                   monitor=monitor, \n",
        "                                                   save_best_only=True,\n",
        "                                                   verbose=0),\n",
        "                tf.keras.callbacks.EarlyStopping(monitor=monitor, \n",
        "                                                 patience=patience,\n",
        "                                                 restore_best_weights=True)\n",
        "                ]\n",
        "    else:\n",
        "        return [tf.keras.callbacks.EarlyStopping(monitor=monitor, \n",
        "                                                 patience=patience,\n",
        "                                                 restore_best_weights=True)\n",
        "                ]\n"
      ],
      "id": "ixfE0G-16VCW",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moIL1DuO8kaq"
      },
      "source": [
        "## Build two ImageDataGenerator to do the on the fly data augmentation for the training"
      ],
      "id": "moIL1DuO8kaq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrAPGtz68j_7",
        "outputId": "c7b0bf36-58b5-45f2-883f-e1583af8a51b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_size = 16\n",
        "input_shape = (150,150,3)\n",
        "\n",
        "\n",
        "# this is the augmentation configuration we will use for training\n",
        "train_datagen = ImageDataGenerator(rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.1,\n",
        "                                   horizontal_flip=True,\n",
        "                                   vertical_flip = True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "# this is the augmentation configuration we will use for testing:\n",
        "# only rescaling\n",
        "test_datagen = ImageDataGenerator()\n",
        "\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(baseline_dir+'train_clahe/',  # this is the target directory\n",
        "                                                   target_size=input_shape[:-1],  # all images will be resized to 150x150\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   class_mode='categorical',\n",
        "                                                   color_mode='rgb',\n",
        "                                                   seed= 42)  #useless for the no augmentation \n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(baseline_dir+'validation_clahe/',  # this is the target directory\n",
        "                                                   target_size=input_shape[:-1],  # all images will be resized to 150x150\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   class_mode='categorical',\n",
        "                                                   color_mode='rgb',\n",
        "                                                   seed= 42)  #useless for the no augmentation \n",
        "\n",
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size"
      ],
      "id": "QrAPGtz68j_7",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 493 images belonging to 4 classes.\n",
            "Found 126 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g31Eh4Gr9QaH"
      },
      "source": [
        "## Use transfer learning"
      ],
      "id": "g31Eh4Gr9QaH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMSHk_en6rZI",
        "outputId": "483612e0-0a5f-40db-e2c3-193624d9a808",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "base_xception = tf.keras.applications.Xception( include_top=False, weights='imagenet', input_shape=input_shape, classes=len(baseline_species) )\n",
        "\n",
        "model_xception = create_cnn(input_shape=input_shape, output_length=len(baseline_species),\n",
        "               model_transfert = base_xception, fine_tune = False, \n",
        "               nb_FC_layer = 2, nb_FC_neurons = 512, reducing = True, activation_FC = 'relu',\n",
        "               dropout = False,\n",
        "               name='my_xception'\n",
        "               )\n",
        "model_xception.summary()"
      ],
      "id": "PMSHk_en6rZI",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n",
            "Model: \"my_xception\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "Rescaling_layer (Rescaling)  (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "xception (Functional)        (None, 5, 5, 2048)        20861480  \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 51200)             0         \n",
            "_________________________________________________________________\n",
            "FC_layer_1 (Dense)           (None, 512)               26214912  \n",
            "_________________________________________________________________\n",
            "FC_layer_2 (Dense)           (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "Output_layer (Dense)         (None, 4)                 1028      \n",
            "=================================================================\n",
            "Total params: 47,208,748\n",
            "Trainable params: 26,347,268\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kinLhVi_EHCX"
      },
      "source": [
        "## Create the space of hyper para tuning and a function to automatically create and build the model\n",
        "* Dropout (between 0 and 0.3)\n",
        "* Num FC layers (between 1 and 3)\n",
        "* Optimizers (Adam, SGD, RMSProp)\n",
        "* Learning rate (between 1e-6 and 1e-1)\n",
        "\n",
        "\n"
      ],
      "id": "kinLhVi_EHCX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGGswbW19Y_G"
      },
      "source": [
        "dim_learning_rate = Real(low=1e-6, high=1e-2, prior='log-uniform',\n",
        "                         name='learning_rate')\n",
        "\n",
        "dim_num_dense_layers = Integer(low=1, high=3, name='num_dense_layers') \n",
        "\n",
        "dim_name_optimizer = Categorical(categories=['adam', 'SGD', 'RMSProp'],\n",
        "                             name='name_optimizer')\n",
        "\n",
        "dim_dropout = Real(low=0, high=0.3,     \n",
        "                         name='dropout')\n"
      ],
      "id": "NGGswbW19Y_G",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MzMC9tdFNaF"
      },
      "source": [
        "dimensions = [dim_learning_rate,\n",
        "              dim_num_dense_layers,\n",
        "              dim_name_optimizer,\n",
        "              dim_dropout]\n",
        "\n",
        "default_parameters = [1e-5, 1, 'adam', 0]"
      ],
      "id": "8MzMC9tdFNaF",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMlapubzGQtm"
      },
      "source": [
        "def create_model(learning_rate, num_dense_layers,\n",
        "                 name_optimizer, dropout):\n",
        "    \"\"\"\n",
        "    Hyper-parameters:\n",
        "    learning_rate:     Learning-rate for the optimizer.\n",
        "    num_dense_layers:  Number of dense layers.\n",
        "    name_optimizer:   Number of nodes in each dense layer.\n",
        "    dropout:        Activation function for all layers.\n",
        "    \"\"\"\n",
        "\n",
        "    base = tf.keras.applications.Xception( include_top=False, weights='imagenet', input_shape=input_shape, classes=len(baseline_species) )\n",
        "\n",
        "    model = create_cnn(input_shape=input_shape, output_length=len(baseline_species),\n",
        "               model_transfert = base, fine_tune = False, \n",
        "               nb_FC_layer = num_dense_layers, nb_FC_neurons = 512, reducing = True, activation_FC = 'relu',\n",
        "               dropout = dropout/10,\n",
        "               name='my_xception'\n",
        "               )\n",
        "    \n",
        "    if name_optimizer == 'adam':\n",
        "      optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    elif name_optimizer == 'SGD':\n",
        "      optmizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    elif name_optimizer == 'RMSProp' :\n",
        "      optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "    else :\n",
        "      print('optimizer {} not known, using RMSProp'.format(name_optimizer))\n",
        "      optimizer =tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "id": "jMlapubzGQtm",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opu6IfrjHbOg"
      },
      "source": [
        "path_best_model = model_dir+ 'best_model.h5'\n",
        "best_accuracy = 0.0"
      ],
      "id": "opu6IfrjHbOg",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiiRo4E9HegY"
      },
      "source": [
        "#Function taken from https://colab.research.google.com/github/Hvass-Labs/TensorFlow-Tutorials/blob/master/19_Hyper-Parameters.ipynb#scrollTo=RWnqMCXmGE5X\n",
        "@use_named_args(dimensions=dimensions)\n",
        "def fitness(learning_rate, num_dense_layers,\n",
        "             name_optimizer, dropout):\n",
        "    \"\"\"\n",
        "    Hyper-parameters:\n",
        "    learning_rate:     Learning-rate for the optimizer.\n",
        "    num_dense_layers:  Number of dense layers.\n",
        "    num_dense_nodes:   Number of nodes in each dense layer.\n",
        "    activation:        Activation function for all layers.\n",
        "    \"\"\"\n",
        "\n",
        "    # Print the hyper-parameters.\n",
        "    print('learning rate: {0:.1e}'.format(learning_rate))\n",
        "    print('num_dense_layers:', num_dense_layers)\n",
        "    print('name_optimizer:', name_optimizer)\n",
        "    print('dropout:', dropout)\n",
        "    print()\n",
        "    \n",
        "    # Create the neural network with these hyper-parameters.\n",
        "    model = create_model(learning_rate=learning_rate, \n",
        "                         num_dense_layers=num_dense_layers,\n",
        "                         name_optimizer=name_optimizer, \n",
        "                         dropout = dropout)\n",
        "\n",
        "  \n",
        "   \n",
        "    # Use Keras to train the model.\n",
        "    history = model.fit_generator(generator=train_generator,\n",
        "                                  steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "                                  validation_data=validation_generator,\n",
        "                                  validation_steps=STEP_SIZE_VALID,\n",
        "                                  epochs=25,\n",
        "                                  callbacks=get_callbacks(patience = 15)\n",
        "                                  )\n",
        "\n",
        "    # Get the classification accuracy on the validation-set\n",
        "    # after the last training-epoch.\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "\n",
        "    # Print the classification accuracy.\n",
        "    print()\n",
        "    print(\"Accuracy: {0:.2%}\".format(accuracy))\n",
        "    print()\n",
        "\n",
        "    # Save the model if it improves on the best-found performance.\n",
        "    # We use the global keyword so we update the variable outside\n",
        "    # of this function.\n",
        "    global best_accuracy\n",
        "\n",
        "    # If the classification accuracy of the saved model is improved ...\n",
        "    if accuracy > best_accuracy:\n",
        "        # Save the new model to harddisk.\n",
        "        model.save(path_best_model)\n",
        "        \n",
        "        # Update the classification accuracy.\n",
        "        best_accuracy = accuracy\n",
        "\n",
        "    # Delete the Keras model with these hyper-parameters from memory.\n",
        "    del model\n",
        "    \n",
        "    # Clear the Keras session, otherwise it will keep adding new\n",
        "    # models to the same TensorFlow graph each time we create\n",
        "    # a model with a different set of hyper-parameters.\n",
        "    K.clear_session()\n",
        "    \n",
        "    # NOTE: Scikit-optimize does minimization so it tries to\n",
        "    # find a set of hyper-parameters with the LOWEST fitness-value.\n",
        "    # Because we are interested in the HIGHEST classification\n",
        "    # accuracy, we need to negate this number so it can be minimized.\n",
        "    return -accuracy"
      ],
      "id": "FiiRo4E9HegY",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QdNOosPLoKd"
      },
      "source": [
        "## Lets do a test run"
      ],
      "id": "4QdNOosPLoKd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HitShkC8J5YP",
        "outputId": "f2d67723-f364-4b6e-8974-683748fdda71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fitness(x=default_parameters)"
      ],
      "id": "HitShkC8J5YP",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate: 1.0e-05\n",
            "num_dense_layers: 1\n",
            "name_optimizer: adam\n",
            "dropout: 0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "30/30 [==============================] - 131s 4s/step - loss: 1.4525 - accuracy: 0.3808 - val_loss: 0.4625 - val_accuracy: 0.7946\n",
            "Epoch 2/25\n",
            "30/30 [==============================] - 6s 207ms/step - loss: 0.8027 - accuracy: 0.6633 - val_loss: 0.4109 - val_accuracy: 0.8304\n",
            "Epoch 3/25\n",
            "30/30 [==============================] - 6s 198ms/step - loss: 0.6830 - accuracy: 0.7123 - val_loss: 0.3384 - val_accuracy: 0.8482\n",
            "Epoch 4/25\n",
            "30/30 [==============================] - 6s 198ms/step - loss: 0.7428 - accuracy: 0.6596 - val_loss: 0.4116 - val_accuracy: 0.8214\n",
            "Epoch 5/25\n",
            "30/30 [==============================] - 6s 198ms/step - loss: 0.6075 - accuracy: 0.7181 - val_loss: 0.4224 - val_accuracy: 0.7768\n",
            "Epoch 6/25\n",
            "30/30 [==============================] - 6s 197ms/step - loss: 0.5341 - accuracy: 0.7850 - val_loss: 0.4871 - val_accuracy: 0.7946\n",
            "Epoch 7/25\n",
            "30/30 [==============================] - 6s 197ms/step - loss: 0.5221 - accuracy: 0.7646 - val_loss: 0.4752 - val_accuracy: 0.7857\n",
            "Epoch 8/25\n",
            "30/30 [==============================] - 6s 197ms/step - loss: 0.5305 - accuracy: 0.7839 - val_loss: 0.4158 - val_accuracy: 0.8036\n",
            "Epoch 9/25\n",
            "30/30 [==============================] - 6s 197ms/step - loss: 0.4591 - accuracy: 0.7971 - val_loss: 0.4173 - val_accuracy: 0.7857\n",
            "Epoch 10/25\n",
            "30/30 [==============================] - 6s 193ms/step - loss: 0.5164 - accuracy: 0.7706 - val_loss: 0.4553 - val_accuracy: 0.7589\n",
            "Epoch 11/25\n",
            "30/30 [==============================] - 6s 199ms/step - loss: 0.5004 - accuracy: 0.7699 - val_loss: 0.4214 - val_accuracy: 0.7946\n",
            "Epoch 12/25\n",
            "30/30 [==============================] - 6s 198ms/step - loss: 0.4194 - accuracy: 0.8126 - val_loss: 0.4069 - val_accuracy: 0.8036\n",
            "Epoch 13/25\n",
            "30/30 [==============================] - 6s 195ms/step - loss: 0.5050 - accuracy: 0.7910 - val_loss: 0.3436 - val_accuracy: 0.8482\n",
            "Epoch 14/25\n",
            "30/30 [==============================] - 6s 195ms/step - loss: 0.4014 - accuracy: 0.8194 - val_loss: 0.3872 - val_accuracy: 0.8125\n",
            "Epoch 15/25\n",
            "30/30 [==============================] - 6s 198ms/step - loss: 0.4015 - accuracy: 0.8105 - val_loss: 0.4186 - val_accuracy: 0.7946\n",
            "Epoch 16/25\n",
            "30/30 [==============================] - 6s 202ms/step - loss: 0.3976 - accuracy: 0.8199 - val_loss: 0.4048 - val_accuracy: 0.8125\n",
            "Epoch 17/25\n",
            "30/30 [==============================] - 6s 199ms/step - loss: 0.3796 - accuracy: 0.8358 - val_loss: 0.3957 - val_accuracy: 0.7857\n",
            "Epoch 18/25\n",
            "30/30 [==============================] - 6s 202ms/step - loss: 0.3848 - accuracy: 0.8138 - val_loss: 0.4722 - val_accuracy: 0.7500\n",
            "Epoch 19/25\n",
            "30/30 [==============================] - 6s 195ms/step - loss: 0.4464 - accuracy: 0.8118 - val_loss: 0.4724 - val_accuracy: 0.7500\n",
            "Epoch 20/25\n",
            "30/30 [==============================] - 6s 194ms/step - loss: 0.3826 - accuracy: 0.8193 - val_loss: 0.4531 - val_accuracy: 0.7857\n",
            "Epoch 21/25\n",
            "30/30 [==============================] - 6s 192ms/step - loss: 0.4130 - accuracy: 0.8073 - val_loss: 0.4771 - val_accuracy: 0.8036\n",
            "Epoch 22/25\n",
            "30/30 [==============================] - 6s 195ms/step - loss: 0.3526 - accuracy: 0.8393 - val_loss: 0.4981 - val_accuracy: 0.7589\n",
            "Epoch 23/25\n",
            "30/30 [==============================] - 6s 193ms/step - loss: 0.3608 - accuracy: 0.8352 - val_loss: 0.4707 - val_accuracy: 0.7500\n",
            "Epoch 24/25\n",
            "30/30 [==============================] - 6s 192ms/step - loss: 0.3701 - accuracy: 0.8358 - val_loss: 0.3907 - val_accuracy: 0.7857\n",
            "Epoch 25/25\n",
            "30/30 [==============================] - 6s 192ms/step - loss: 0.3878 - accuracy: 0.8372 - val_loss: 0.4477 - val_accuracy: 0.7768\n",
            "\n",
            "Accuracy: 77.68%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.7767857313156128"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt0r_gYZMhrX"
      },
      "source": [
        "# Lets run the full hyperpara search"
      ],
      "id": "mt0r_gYZMhrX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pnu4Y33oL_Uo",
        "outputId": "111c1ea2-4f7a-481f-b7b2-f11a140cf3e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "search_result = gp_minimize(func=fitness,\n",
        "                            dimensions=dimensions,\n",
        "                            acq_func='EI', # Expected Improvement.\n",
        "                            n_calls=40,\n",
        "                            x0=default_parameters)"
      ],
      "id": "Pnu4Y33oL_Uo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate: 1.0e-05\n",
            "num_dense_layers: 1\n",
            "name_optimizer: adam\n",
            "dropout: 0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "30/30 [==============================] - 9s 231ms/step - loss: 1.3982 - accuracy: 0.4033 - val_loss: 0.4608 - val_accuracy: 0.8125\n",
            "Epoch 2/25\n",
            "30/30 [==============================] - 6s 211ms/step - loss: 0.7281 - accuracy: 0.6717 - val_loss: 0.4585 - val_accuracy: 0.7679\n",
            "Epoch 3/25\n",
            "30/30 [==============================] - 6s 197ms/step - loss: 0.6699 - accuracy: 0.7133 - val_loss: 0.4736 - val_accuracy: 0.7589\n",
            "Epoch 4/25\n",
            "30/30 [==============================] - 6s 200ms/step - loss: 0.6411 - accuracy: 0.6797 - val_loss: 0.5503 - val_accuracy: 0.6964\n",
            "Epoch 5/25\n",
            "30/30 [==============================] - 6s 193ms/step - loss: 0.6206 - accuracy: 0.6951 - val_loss: 0.4616 - val_accuracy: 0.7946\n",
            "Epoch 6/25\n",
            "30/30 [==============================] - 6s 195ms/step - loss: 0.5096 - accuracy: 0.7979 - val_loss: 0.4387 - val_accuracy: 0.7500\n",
            "Epoch 7/25\n",
            "30/30 [==============================] - 6s 196ms/step - loss: 0.5017 - accuracy: 0.7808 - val_loss: 0.4368 - val_accuracy: 0.7768\n",
            "Epoch 8/25\n",
            "30/30 [==============================] - 6s 195ms/step - loss: 0.4990 - accuracy: 0.7798 - val_loss: 0.4072 - val_accuracy: 0.7768\n",
            "Epoch 9/25\n",
            "30/30 [==============================] - 6s 190ms/step - loss: 0.4570 - accuracy: 0.7936 - val_loss: 0.4053 - val_accuracy: 0.8036\n",
            "Epoch 10/25\n",
            "30/30 [==============================] - 6s 191ms/step - loss: 0.4612 - accuracy: 0.7947 - val_loss: 0.4115 - val_accuracy: 0.7589\n",
            "Epoch 11/25\n",
            "30/30 [==============================] - 6s 193ms/step - loss: 0.4454 - accuracy: 0.7998 - val_loss: 0.4311 - val_accuracy: 0.7589\n",
            "Epoch 12/25\n",
            "30/30 [==============================] - 6s 194ms/step - loss: 0.4494 - accuracy: 0.8104 - val_loss: 0.3940 - val_accuracy: 0.7946\n",
            "Epoch 13/25\n",
            "30/30 [==============================] - 6s 197ms/step - loss: 0.4493 - accuracy: 0.8066 - val_loss: 0.4870 - val_accuracy: 0.7679\n",
            "Epoch 14/25\n",
            "30/30 [==============================] - 6s 193ms/step - loss: 0.4473 - accuracy: 0.8117 - val_loss: 0.4236 - val_accuracy: 0.7857\n",
            "Epoch 15/25\n",
            "30/30 [==============================] - 6s 194ms/step - loss: 0.4415 - accuracy: 0.8061 - val_loss: 0.4483 - val_accuracy: 0.7768\n",
            "Epoch 16/25\n",
            "30/30 [==============================] - 6s 196ms/step - loss: 0.4012 - accuracy: 0.8256 - val_loss: 0.4537 - val_accuracy: 0.8036\n",
            "Epoch 17/25\n",
            "30/30 [==============================] - 6s 196ms/step - loss: 0.4484 - accuracy: 0.8052 - val_loss: 0.4736 - val_accuracy: 0.7857\n",
            "Epoch 18/25\n",
            "30/30 [==============================] - 6s 203ms/step - loss: 0.3290 - accuracy: 0.8705 - val_loss: 0.4117 - val_accuracy: 0.7946\n",
            "Epoch 19/25\n",
            "30/30 [==============================] - 6s 194ms/step - loss: 0.3947 - accuracy: 0.8189 - val_loss: 0.4259 - val_accuracy: 0.7946\n",
            "Epoch 20/25\n",
            "30/30 [==============================] - 6s 192ms/step - loss: 0.3988 - accuracy: 0.8295 - val_loss: 0.4350 - val_accuracy: 0.7946\n",
            "Epoch 21/25\n",
            "30/30 [==============================] - 6s 189ms/step - loss: 0.3816 - accuracy: 0.8351 - val_loss: 0.4657 - val_accuracy: 0.7768\n",
            "Epoch 22/25\n",
            "16/30 [===============>..............] - ETA: 2s - loss: 0.3775 - accuracy: 0.8354"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlJ9DGL8NRuy"
      },
      "source": [
        ""
      ],
      "id": "mlJ9DGL8NRuy",
      "execution_count": null,
      "outputs": []
    }
  ]
}